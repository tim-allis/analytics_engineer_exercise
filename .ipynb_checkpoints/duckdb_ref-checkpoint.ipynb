{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff26ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import os\n",
    "import csv\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f62ee",
   "metadata": {},
   "source": [
    "# DuckDB commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection to a database\n",
    "#conn = duckdb.connect('duckdb/stock.db')\n",
    "\n",
    "\n",
    "# drop a table\n",
    "# conn.execute(\"DROP TABLE stock_init_header;\")\n",
    "\n",
    "\n",
    "# Drop a view if it exists\n",
    "# conn.execute(\"DROP VIEW IF EXISTS stock_init_header\")\n",
    "\n",
    "\n",
    "\n",
    "# get information about the columns\n",
    "# result = conn.execute(\"PRAGMA table_info(amazon_weekly_search);\")\n",
    "# rows = result.fetchall()\n",
    "# for row in rows:\n",
    "#     print(row)\n",
    "\n",
    "\n",
    "\n",
    "# create_table_from_file = '''\n",
    "# CREATE TABLE amazon_weekly_search AS SELECT * FROM read_csv_auto(\"data/uk_2023_search.csv\")\n",
    "# '''\n",
    "# conn.execute(create_table_from_file)\n",
    "\n",
    "\n",
    "\n",
    "# sql_create_table = \"\"\"\n",
    "#   CREATE TABLE amazon_weekly_search_export AS\n",
    "#   SELECT \n",
    "#     CAST(column19 AS VARCHAR) AS search_frequency_rank,\n",
    "#     CAST(column01 AS VARCHAR) AS search_term,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_brand_no__1,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_brand_no__2,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_brand_no__3,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_category_no__1,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_category_no__2,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_category_no__3,\n",
    "#     CAST(column02 AS VARCHAR) AS top_clicked_product_no__1___asin,\n",
    "#     CAST(column03 AS VARCHAR) AS top_clicked_product_no__1___product_title,\n",
    "#     CAST(column04 AS VARCHAR) AS top_clicked_product_no__1___click_share,\n",
    "#     CAST(column05 AS VARCHAR) AS top_clicked_product_no__1___conversion_share,\n",
    "#     CAST(column06 AS VARCHAR) AS top_clicked_product_no__2___asin,\n",
    "#     CAST(column07 AS VARCHAR) AS top_clicked_product_no__2___product_title,\n",
    "#     CAST(column08 AS VARCHAR) AS top_clicked_product_no__2___click_share,\n",
    "#     CAST(column09 AS VARCHAR) AS top_clicked_product_no__2___conversion_share,\n",
    "#     CAST(column10 AS VARCHAR) AS top_clicked_product_no__3___asin,\n",
    "#     CAST(column11 AS VARCHAR) AS top_clicked_product_no__3___product_title,\n",
    "#     CAST(column12 AS VARCHAR) AS top_clicked_product_no__3___click_share,\n",
    "#     CAST(column13 AS VARCHAR) AS top_clicked_product_no__3___conversion_share,\n",
    "#     CAST('' AS VARCHAR) AS reporting_date,\n",
    "#     CAST(column14 AS VARCHAR) AS country,\n",
    "#     CAST(column16 AS VARCHAR) AS weekno,\n",
    "#     CAST(column15 AS VARCHAR) AS year    \n",
    "#   FROM \n",
    "#     amazon_weekly_search   \n",
    "# \"\"\"\n",
    "# conn.execute(sql_create_table)\n",
    "\n",
    "\n",
    "\n",
    "# result = conn.execute(\"SELECT COUNT(*) FROM amazon_weekly_search\")\n",
    "# print(result.fetchall())\n",
    "\n",
    "\n",
    "\n",
    "# result = conn.execute(\"SELECT COUNT(*) FROM amazon_weekly_search_export\")\n",
    "# print(result.fetchall())\n",
    "\n",
    "\n",
    "\n",
    "# # create a table from a df\n",
    "# # temp view from dataframe \n",
    "# conn.register('stock_init_header_temp', df_header)\n",
    "# # Save as a permanent table\n",
    "# conn.execute(\"CREATE TABLE stock_init_header AS SELECT * FROM stock_init_header_temp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c599e33",
   "metadata": {},
   "source": [
    "# duckdb connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a8a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to duckdb\n",
    "\n",
    "conn = duckdb.connect('duckdb/stock.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5dda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_init\n",
      "stock_init_header\n"
     ]
    }
   ],
   "source": [
    "# list tables in databse\n",
    "\n",
    "sql_list_tables = \"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables    \n",
    "\"\"\"\n",
    "result = conn.execute(sql_list_tables)\n",
    "tables = result.fetchall()\n",
    "for table in tables:\n",
    "    print(table[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce78c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a new table from a file\n",
    "\n",
    "# create_table_from_file = '''\n",
    "# CREATE TABLE stock_init AS SELECT * FROM read_csv_auto(\"data/ZSTOCKDET_20240306154916.csv\")\n",
    "# '''\n",
    "# conn.execute(create_table_from_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a874d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18177898,)]\n"
     ]
    }
   ],
   "source": [
    "# check table count\n",
    "\n",
    "result = conn.execute(\"SELECT COUNT(*) FROM stock_init\")\n",
    "print(result.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d479f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in header file\n",
    "\n",
    "# header_file = 'S_ZSTOCKDET_20240306154916.csv'\n",
    "# file_path = 'data/'\n",
    "# filepath_header = os.path.join(file_path, header_file)\n",
    "# with open(filepath_header) as tf:    \n",
    "#     col_count = [len(l.split(\",\")) for l in tf.readlines()]\n",
    "#     col_names = [i for i in range(0, max(col_count))]\n",
    "#     df_header = pd.read_csv(filepath_header, header=None, delimiter=\",\", names=col_names, dtype='str')\n",
    "#     df_header = df_header.iloc[3:]\n",
    "#     df_header.columns = df_header.iloc[0]\n",
    "#     df_header = df_header[1:]\n",
    "#     col_list = df_header['FIELDNAME'].tolist()\n",
    "#     df_header = pd.DataFrame(columns = col_list)\n",
    "\n",
    "# # add at least 1 row to the table\n",
    "# new_row = df_header.columns.tolist()\n",
    "# df_header.loc[-1] = new_row\n",
    "# df_header.index = df_header.index + 1  \n",
    "# df_header = df_header.sort_index()  \n",
    "\n",
    "# print(df_header.shape)\n",
    "\n",
    "# # temp view from dataframe \n",
    "# conn.register('stock_init_header_temp', df_header)\n",
    "\n",
    "# # Save as a permanent table\n",
    "# conn.execute(\"CREATE TABLE stock_init_header AS SELECT * FROM stock_init_header_temp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c794d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,)]\n"
     ]
    }
   ],
   "source": [
    "# check table count\n",
    "\n",
    "result = conn.execute(\"SELECT COUNT(*) FROM stock_init_header\")\n",
    "print(result.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a4a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 1\n",
      "Batch number: 2\n",
      "Batch number: 3\n",
      "Batch number: 4\n",
      "Batch number: 5\n",
      "Batch number: 6\n",
      "Batch number: 7\n",
      "Batch number: 8\n",
      "Batch number: 9\n",
      "Batch number: 10\n",
      "Batch number: 11\n",
      "Batch number: 12\n",
      "Batch number: 13\n",
      "Batch number: 14\n",
      "Batch number: 15\n",
      "Batch number: 16\n",
      "Batch number: 17\n",
      "Batch number: 18\n",
      "Batch number: 19\n",
      "Batch number: 20\n",
      "Batch number: 21\n",
      "Batch number: 22\n",
      "Batch number: 23\n",
      "Batch number: 24\n",
      "Batch number: 25\n",
      "Batch number: 26\n",
      "Batch number: 27\n",
      "Batch number: 28\n",
      "Batch number: 29\n",
      "Batch number: 30\n",
      "Batch number: 31\n",
      "Batch number: 32\n",
      "Batch number: 33\n",
      "Batch number: 34\n",
      "Batch number: 35\n",
      "Batch number: 36\n",
      "Batch number: 37\n",
      "Batch number: 38\n",
      "Batch number: 39\n",
      "Batch number: 40\n",
      "Batch number: 41\n",
      "Batch number: 42\n",
      "Batch number: 43\n",
      "Batch number: 44\n",
      "Batch number: 45\n",
      "Batch number: 46\n",
      "Batch number: 47\n",
      "Batch number: 48\n",
      "Batch number: 49\n",
      "Batch number: 50\n",
      "Batch number: 51\n",
      "Batch number: 52\n",
      "Batch number: 53\n",
      "Batch number: 54\n",
      "Batch number: 55\n",
      "Batch number: 56\n",
      "Batch number: 57\n",
      "Batch number: 58\n",
      "Batch number: 59\n",
      "Batch number: 60\n",
      "Batch number: 61\n",
      "Batch number: 62\n",
      "Batch number: 63\n",
      "Batch number: 64\n",
      "Batch number: 65\n",
      "Batch number: 66\n",
      "Batch number: 67\n",
      "Batch number: 68\n",
      "Batch number: 69\n",
      "Batch number: 70\n",
      "Batch number: 71\n",
      "Batch number: 72\n",
      "Batch number: 73\n",
      "Batch number: 74\n",
      "Batch number: 75\n",
      "Batch number: 76\n",
      "Batch number: 77\n",
      "Batch number: 78\n",
      "Batch number: 79\n",
      "Batch number: 80\n",
      "Batch number: 81\n",
      "Batch number: 82\n",
      "Batch number: 83\n",
      "Batch number: 84\n",
      "Batch number: 85\n",
      "Batch number: 86\n",
      "Batch number: 87\n",
      "Batch number: 88\n",
      "Batch number: 89\n",
      "Batch number: 90\n",
      "Batch number: 91\n",
      "Batch number: 92\n",
      "Batch number: 93\n",
      "Batch number: 94\n",
      "Batch number: 95\n",
      "Batch number: 96\n",
      "Batch number: 97\n",
      "Batch number: 98\n",
      "Batch number: 99\n",
      "Batch number: 100\n",
      "Batch number: 101\n",
      "Batch number: 102\n",
      "Batch number: 103\n",
      "Batch number: 104\n",
      "Batch number: 105\n",
      "Batch number: 106\n",
      "Batch number: 107\n",
      "Batch number: 108\n",
      "Batch number: 109\n",
      "Batch number: 110\n",
      "Batch number: 111\n",
      "Batch number: 112\n",
      "Batch number: 113\n",
      "Batch number: 114\n",
      "Batch number: 115\n",
      "Batch number: 116\n",
      "Batch number: 117\n",
      "Batch number: 118\n",
      "Batch number: 119\n",
      "Batch number: 120\n",
      "Batch number: 121\n",
      "Batch number: 122\n",
      "Batch number: 123\n",
      "Batch number: 124\n",
      "Batch number: 125\n",
      "Batch number: 126\n",
      "Batch number: 127\n",
      "Batch number: 128\n",
      "Batch number: 129\n",
      "Batch number: 130\n",
      "Batch number: 131\n",
      "Batch number: 132\n",
      "Batch number: 133\n",
      "Batch number: 134\n",
      "Batch number: 135\n",
      "Batch number: 136\n",
      "Batch number: 137\n",
      "Batch number: 138\n",
      "Batch number: 139\n",
      "Batch number: 140\n",
      "Batch number: 141\n",
      "Batch number: 142\n",
      "Batch number: 143\n",
      "Batch number: 144\n",
      "Batch number: 145\n",
      "Batch number: 146\n",
      "Batch number: 147\n",
      "Batch number: 148\n",
      "Batch number: 149\n",
      "Batch number: 150\n",
      "Batch number: 151\n",
      "Batch number: 152\n",
      "Batch number: 153\n",
      "Batch number: 154\n",
      "Batch number: 155\n",
      "Batch number: 156\n",
      "Batch number: 157\n",
      "Batch number: 158\n",
      "Batch number: 159\n",
      "Batch number: 160\n",
      "Batch number: 161\n",
      "Batch number: 162\n",
      "Batch number: 163\n",
      "Batch number: 164\n",
      "Batch number: 165\n",
      "Batch number: 166\n",
      "Batch number: 167\n",
      "Batch number: 168\n",
      "Batch number: 169\n",
      "Batch number: 170\n",
      "Batch number: 171\n",
      "Batch number: 172\n",
      "Batch number: 173\n",
      "Batch number: 174\n",
      "Batch number: 175\n",
      "Batch number: 176\n",
      "Batch number: 177\n",
      "Batch number: 178\n",
      "Batch number: 179\n",
      "Batch number: 180\n",
      "Batch number: 181\n",
      "Batch number: 182\n"
     ]
    }
   ],
   "source": [
    "# export the results of the query to csv\n",
    "\n",
    "output_file_name = 'stock_movements_delta_init_8000.csv'\n",
    "inserted_timestamp = pd.Timestamp.now()\n",
    "\n",
    "sql_export_data_ = f\"\"\"  \n",
    "  SELECT \n",
    "    *,\n",
    "    '{output_file_name}' AS filename,\n",
    "    '{inserted_timestamp}' AS inserted_timestamp\n",
    "  FROM \n",
    "    stock_init_header  \n",
    "  WHERE\n",
    "    CALDAY != 'CALDAY'\n",
    "  UNION ALL\n",
    "  SELECT \n",
    "    *,\n",
    "    '{output_file_name}' AS filename,\n",
    "    '{inserted_timestamp}' AS inserted_timestamp\n",
    "  FROM \n",
    "    stock_init\n",
    "  --LIMIT 10000\n",
    "\"\"\"\n",
    "\n",
    "result = conn.execute(sql_export_data_)\n",
    "\n",
    "output_file = f'output/{output_file_name}'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8-sig', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([col[0] for col in result.description])\n",
    "    batch_size = 100000  \n",
    "    batch_counter = 0\n",
    "    while True:\n",
    "        rows = result.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        batch_counter += 1\n",
    "        print(f'Batch number: {batch_counter}')\n",
    "        csv_writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75114fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21b43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a df from the header and data tables\n",
    "\n",
    "# sql_select = f\"\"\"\n",
    "#   SELECT \n",
    "#     *\n",
    "#   FROM \n",
    "#     stock_init_header  \n",
    "#   WHERE\n",
    "#     CALDAY != 'CALDAY'\n",
    "#   UNION ALL\n",
    "#   SELECT \n",
    "#     *\n",
    "#   FROM \n",
    "#     stock_init\n",
    "#   LIMIT 10  \n",
    "# \"\"\"\n",
    "\n",
    "# df = conn.execute(sql_select).df()\n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = conn.execute(\"SELECT * FROM stock_export\")\n",
    "\n",
    "# output_file = 'stock_delta_8000_test.csv'\n",
    "\n",
    "# # Open a CSV file for writing\n",
    "# with open(output_file, 'w', newline='') as csvfile:\n",
    "#     # Create a CSV writer object\n",
    "#     csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "#     # Write the header row\n",
    "#     csv_writer.writerow([col[0] for col in result.description])\n",
    "    \n",
    "#     # Write the data rows\n",
    "#     csv_writer.writerows(result.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbe501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_year = '2023'\n",
    "# v_week = '11'\n",
    "\n",
    "# sql_export = f\"\"\"\n",
    "#   SELECT \n",
    "#     *\n",
    "#   FROM \n",
    "#     amazon_weekly_search_export   \n",
    "#   WHERE\n",
    "#     year = {v_year}\n",
    "#     AND weekno = {v_week}\n",
    "#   ORDER BY \n",
    "#     CAST(weekno AS NUMERIC)\n",
    "#   LIMIT 100\n",
    "# \"\"\"\n",
    "\n",
    "# result = conn.execute(sql_export)\n",
    "# rows = result.fetchall()\n",
    "\n",
    "# csv_file_path = f\"data_output/search_terms_weeklyhistory_uk_{v_week}_{v_year}.csv\"\n",
    "\n",
    "\n",
    "# with open(csv_file_path, 'w', encoding='utf-8') as f:\n",
    "#     f.write(','.join([str(col[0]) for col in result.description]) + '\\n')    \n",
    "#     for row in rows:\n",
    "#         f.write(','.join([str(value) for value in row]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_header_row(year, week):    \n",
    "    \n",
    "#     col1 = 'Programme=[Retail]'\n",
    "#     col2 = 'Distributor View=[Manufacturing]'\n",
    "#     col3 = 'Countries=[UK]'\n",
    "#     col4 =  f'Businesses=[]'\n",
    "#     col5 = 'Locale=[en_GB]'\n",
    "#     col6 = 'Search for ASINs=[]'\n",
    "#     col7 = 'Reporting Range=[Week]'    \n",
    "#     dfrom, dto = get_start_end_dates(year, week)\n",
    "#     col8 = f'Viewing=[{dfrom} - {dto}]'    \n",
    "#     header_col_list = [col1, col2, col3, col4, col5, col6, col7, col8]\n",
    "    \n",
    "#     return header_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c43ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_year = '2023'\n",
    "# v_week = '11'\n",
    "\n",
    "# sql_export = \"\"\"\n",
    "#   SELECT \n",
    "#     *\n",
    "#   FROM \n",
    "#     amazon_weekly_search_export   \n",
    "#   WHERE\n",
    "#     year = '2023'\n",
    "#     AND weekno = '11'\n",
    "#   ORDER BY \n",
    "#     CAST(weekno AS NUMERIC)\n",
    "# \"\"\"\n",
    "\n",
    "# result = conn.execute(sql_export)\n",
    "# rows = result.fetchall()\n",
    "# csv_file_path = \"amazon_search_weekly.csv\"\n",
    "\n",
    "# # Write the data to the CSV file\n",
    "# with open(csv_file_path, 'w', encoding='utf-8') as f:\n",
    "#     # Write header\n",
    "#     f.write(','.join([str(col[0]) for col in result.description]) + '\\n')\n",
    "#     # Write rows\n",
    "#     for row in rows:\n",
    "#         f.write(','.join([str(value) for value in row]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_select = \"\"\"\n",
    "#   SELECT \n",
    "#     *\n",
    "#   FROM \n",
    "#     amazon_weekly_search_export   \n",
    "#   WHERE\n",
    "#     year = '2023'\n",
    "#     AND weekno = '11'\n",
    "#   ORDER BY \n",
    "#     CAST(weekno AS NUMERIC)\n",
    "# \"\"\"\n",
    "\n",
    "# df = conn.execute(sql_select).df()\n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8601c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_select = \"\"\"\n",
    "#   SELECT \n",
    "#     CAST(column19 AS VARCHAR) AS search_frequency_rank,\n",
    "#     CAST(column01 AS VARCHAR) AS search_term,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_brand_no__1,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_brand_no__2,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_brand_no__3,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_category_no__1,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_category_no__2,\n",
    "#     CAST('' AS VARCHAR) AS top_clicked_category_no__3,\n",
    "#     CAST(column02 AS VARCHAR) AS top_clicked_product_no__1___asin,\n",
    "#     CAST(column03 AS VARCHAR) AS top_clicked_product_no__1___product_title,\n",
    "#     CAST(column04 AS VARCHAR) AS top_clicked_product_no__1___click_share,\n",
    "#     CAST(column05 AS VARCHAR) AS top_clicked_product_no__1___conversion_share,\n",
    "#     CAST(column06 AS VARCHAR) AS top_clicked_product_no__2___asin,\n",
    "#     CAST(column07 AS VARCHAR) AS top_clicked_product_no__2___product_title,\n",
    "#     CAST(column08 AS VARCHAR) AS top_clicked_product_no__2___click_share,\n",
    "#     CAST(column09 AS VARCHAR) AS top_clicked_product_no__2___conversion_share,\n",
    "#     CAST(column10 AS VARCHAR) AS top_clicked_product_no__3___asin,\n",
    "#     CAST(column11 AS VARCHAR) AS top_clicked_product_no__3___product_title,\n",
    "#     CAST(column12 AS VARCHAR) AS top_clicked_product_no__3___click_share,\n",
    "#     CAST(column13 AS VARCHAR) AS top_clicked_product_no__3___conversion_share,\n",
    "#     CAST('' AS VARCHAR) AS reporting_date,\n",
    "#     CAST(column14 AS VARCHAR) AS country,\n",
    "#     CAST(column16 AS VARCHAR) AS weekno,\n",
    "#     CAST(column15 AS VARCHAR) AS year    \n",
    "#   FROM \n",
    "#     amazon_weekly_search \n",
    "#   LIMIT 2\n",
    "# \"\"\"\n",
    "\n",
    "# df = conn.execute(sql_select).df()\n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_start_end_dates(year, week):    \n",
    "#     yw = f'{year}-{week}'\n",
    "#     sd = datetime.datetime.strptime(yw + '-1', \"%Y-%W-%w\")    \n",
    "#     ed = sd + datetime.timedelta(days=6)    \n",
    "#     sd = sd.strftime('%d/%m/%Y')\n",
    "#     ed = ed.strftime('%d/%m/%Y')    \n",
    "#     return sd, ed\n",
    "\n",
    "\n",
    "# def get_header_row(v_range, year, week):        \n",
    "#     col1 = F\"Reporting range=['{v_range}']\"\n",
    "#     dfrom, dto = get_start_end_dates(year, week)\n",
    "#     col2 = f\"Select week=['Week {week} | {dfrom} - {dto}']\"    \n",
    "#     header_col_list = [col1, col2]    \n",
    "#     return header_col_list, dto\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
